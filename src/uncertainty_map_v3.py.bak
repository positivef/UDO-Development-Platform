#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Uncertainty Map v3.0 - Predictive & Self-Learning System

Revolutionary Improvements:
1. Predictive uncertainty modeling (예측 가능)
2. Pattern recognition from history (패턴 학습)
3. Auto-mitigation strategies (자동 해결책)
4. Real-time risk scoring (실시간 리스크)
5. Quantum uncertainty states (양자 불확실성)

Author: VibeCoding Team
Date: 2025-11-17
Version: 3.0.0
"""

import hashlib
import json
import logging
import math
import os
import sys
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import numpy as np

from filelock import FileLock

# Windows Unicode 인코딩 문제 근본 해결
if sys.platform == 'win32':
    # 환경변수 설정
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    # stdout/stderr를 UTF-8 모드로 재구성
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')

# ML imports for prediction
logger = logging.getLogger(__name__)


def _get_storage_dir() -> Path:
    env_dir = os.environ.get('UDO_STORAGE_DIR') or os.environ.get('UDO_HOME')
    base_dir = Path(env_dir).expanduser() if env_dir else Path.home() / '.udo'
    base_dir.mkdir(parents=True, exist_ok=True)
    return base_dir


DEFAULT_STORAGE_DIR = _get_storage_dir()

try:
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import StandardScaler
    ML_AVAILABLE = True
except Exception:
    ML_AVAILABLE = False
    logger.warning("ML libraries not available, using fallback prediction")


class UncertaintyState(Enum):
    """Quantum-inspired uncertainty states"""
    DETERMINISTIC = "deterministic"      # 100% certain
    PROBABILISTIC = "probabilistic"      # 70-99% certain
    QUANTUM = "quantum"                   # 40-70% (superposition)
    CHAOTIC = "chaotic"                   # 10-40% (butterfly effect)
    VOID = "void"                         # <10% (unknown territory)


@dataclass
class UncertaintyVector:
    """Multi-dimensional uncertainty representation"""
    technical: float      # 기술적 불확실성
    market: float        # 시장 불확실성
    resource: float      # 리소스 불확실성
    timeline: float      # 일정 불확실성
    quality: float       # 품질 불확실성

    def magnitude(self) -> float:
        """Calculate total uncertainty magnitude"""
        return math.sqrt(
            self.technical**2 + self.market**2 +
            self.resource**2 + self.timeline**2 + self.quality**2
        ) / math.sqrt(5)  # Normalize to 0-1

    def dominant_dimension(self) -> str:
        """Find which dimension has highest uncertainty"""
        dims = {
            'technical': self.technical,
            'market': self.market,
            'resource': self.resource,
            'timeline': self.timeline,
            'quality': self.quality
        }
        return max(dims.items(), key=lambda x: x[1])[0]


@dataclass
class PredictiveModel:
    """Predictive model for uncertainty evolution"""
    trend: str  # "increasing", "decreasing", "stable", "oscillating"
    velocity: float  # Rate of change
    acceleration: float  # Change of rate
    inflection_points: List[datetime]  # When trend changes
    confidence_interval: Tuple[float, float]  # (lower, upper)
    predicted_resolution: Optional[datetime]

    def predict_future(self, hours_ahead: int) -> float:
        """Predict uncertainty level in the future"""
        # Simple linear model with acceleration
        base_change = self.velocity * hours_ahead
        accel_change = 0.5 * self.acceleration * (hours_ahead ** 2)
        return max(0, min(1, base_change + accel_change))


@dataclass
class MitigationStrategy:
    """Auto-generated mitigation strategy"""
    id: str
    uncertainty_id: str
    action: str
    priority: int  # 1-5
    estimated_impact: float  # 0-1
    estimated_cost: float  # hours
    prerequisites: List[str]
    success_probability: float
    fallback_strategy: Optional[str]

    def roi(self) -> float:
        """Calculate return on investment"""
        if self.estimated_cost == 0:
            return float('inf')
        return (self.estimated_impact * self.success_probability) / self.estimated_cost


class UncertaintyMapV3:
    """
    3rd Generation Uncertainty Management System

    Key Features:
    - Predictive modeling
    - Pattern learning
    - Auto-mitigation
    - Risk scoring
    - Quantum states
    """

    def __init__(self, project_name: str):
        self.project_name = project_name
        self.uncertainties: Dict[str, UncertaintyVector] = {}
        self.predictions: Dict[str, PredictiveModel] = {}
        self.mitigations: Dict[str, List[MitigationStrategy]] = {}
        self.patterns: Dict[str, Any] = {}

        # Historical data for learning
        self.storage_dir = DEFAULT_STORAGE_DIR
        self.history_file = self.storage_dir / f"uncertainty_history_{project_name}.json"
        self.load_history()

        # ML models for prediction
        if ML_AVAILABLE:
            self.predictor = RandomForestRegressor(n_estimators=100)
            self.scaler = StandardScaler()
            self.is_trained = False
            self._predictor_feature_count = 0
        else:
            self.predictor = None
            self.scaler = None
            self.is_trained = False
            self._predictor_feature_count = 0

        # Pattern database
        self.known_patterns = self._load_known_patterns()

    def train_predictor(self, features: np.ndarray, labels: np.ndarray) -> None:
        """Train the ML predictor with validation."""
        if not ML_AVAILABLE:
            raise RuntimeError("ML libraries are not available for training")

        if features.ndim != 2:
            raise ValueError("Features must be a 2D array")
        if labels.ndim != 1:
            raise ValueError("Labels must be a 1D array")
        if features.shape[0] == 0:
            raise ValueError("Training data cannot be empty")
        if features.shape[0] != labels.shape[0]:
            raise ValueError("Features and labels must have the same number of samples")

        self.scaler.fit(features)
        scaled_features = self.scaler.transform(features)
        self.predictor.fit(scaled_features, labels)
        self.is_trained = True
        self._predictor_feature_count = features.shape[1]

    def _predict_with_ml(self, vector: UncertaintyVector, hours: int) -> Optional[Tuple[str, float, datetime, float]]:
        """Use the trained ML model for predictions if available."""
        if not (ML_AVAILABLE and getattr(self, 'is_trained', False)):
            return None

        try:
            feature_vector = np.array([
                [
                    vector.technical,
                    vector.market,
                    vector.resource,
                    vector.timeline,
                    vector.quality,
                    hours
                ]
            ])

            if self._predictor_feature_count and feature_vector.shape[1] != self._predictor_feature_count:
                raise ValueError("Feature vector does not match trained model shape")

            scaled_features = self.scaler.transform(feature_vector)
            predicted_magnitude = float(self.predictor.predict(scaled_features)[0])
            current_magnitude = vector.magnitude()
            delta = predicted_magnitude - current_magnitude

            if abs(delta) < 0.01:
                trend = "stable"
            elif delta > 0:
                trend = "increasing"
            else:
                trend = "decreasing"

            velocity = delta / max(hours, 1)
            predicted_resolution = datetime.now() + timedelta(hours=hours)
            return trend, velocity, predicted_resolution, predicted_magnitude
        except Exception as ml_error:
            logger.warning("ML prediction failed, falling back to heuristics: %s", ml_error)
            return None

    def _predict_with_rules(
        self,
        vector: UncertaintyVector,
        pattern: Optional[Dict],
        hours: int
    ) -> Tuple[str, float, datetime, float]:
        """Heuristic prediction used when ML is unavailable."""
        if pattern:
            typical = pattern['typical_vector']
            diff = vector.magnitude() - typical.magnitude()

            if abs(diff) < 0.1:
                trend = "stable"
                velocity = 0.0
            elif diff > 0:
                trend = "increasing"
                velocity = diff / 24
            else:
                trend = "decreasing"
                velocity = diff / 24
        else:
            trend = "decreasing"
            velocity = -0.01

        predicted_resolution = datetime.now() + timedelta(hours=hours * 3)
        predicted_magnitude = max(0.0, min(1.0, vector.magnitude() + velocity * hours))
        return trend, velocity, predicted_resolution, predicted_magnitude

    def _load_known_patterns(self) -> Dict:
        """Load known uncertainty patterns"""
        return {
            "early_stage_high": {
                "description": "High uncertainty in early stages",
                "phases": ["ideation", "design"],
                "typical_vector": UncertaintyVector(0.8, 0.9, 0.6, 0.7, 0.5),
                "resolution_time": 168  # hours
            },
            "implementation_spike": {
                "description": "Uncertainty spike during implementation",
                "phases": ["implementation"],
                "typical_vector": UncertaintyVector(0.7, 0.3, 0.5, 0.8, 0.6),
                "resolution_time": 72
            },
            "testing_convergence": {
                "description": "Uncertainty reduces during testing",
                "phases": ["testing"],
                "typical_vector": UncertaintyVector(0.3, 0.2, 0.2, 0.3, 0.7),
                "resolution_time": 48
            },
            "market_unknown": {
                "description": "Persistent market uncertainty",
                "phases": ["all"],
                "typical_vector": UncertaintyVector(0.3, 0.9, 0.4, 0.5, 0.4),
                "resolution_time": 336
            }
        }

    def analyze_context(self, context: Dict) -> Tuple[UncertaintyVector, UncertaintyState]:
        """
        Analyze context and return uncertainty vector and state
        """
        phase = context.get('phase')
        if not phase:
            raise ValueError("Context must include a phase")
        has_code = len(context.get('files', [])) > 0
        team_size = context.get('team_size', 1)
        if team_size <= 0:
            raise ValueError("team_size must be greater than zero")
        timeline = context.get('timeline_weeks', 12)
        if timeline <= 0:
            raise ValueError("timeline_weeks must be greater than zero")

        # Calculate uncertainty dimensions
        technical = self._calc_technical_uncertainty(phase, has_code)
        market = self._calc_market_uncertainty(phase, context.get('market_validation', 0))
        resource = self._calc_resource_uncertainty(team_size, timeline)
        timeline_unc = self._calc_timeline_uncertainty(phase, timeline)
        quality = self._calc_quality_uncertainty(phase, has_code)

        vector = UncertaintyVector(technical, market, resource, timeline_unc, quality)

        # Determine quantum state
        magnitude = vector.magnitude()
        if magnitude < 0.1:
            state = UncertaintyState.DETERMINISTIC
        elif magnitude < 0.3:
            state = UncertaintyState.PROBABILISTIC
        elif magnitude < 0.6:
            state = UncertaintyState.QUANTUM
        elif magnitude < 0.8:
            state = UncertaintyState.CHAOTIC
        else:
            state = UncertaintyState.VOID

        return vector, state

    def _calc_technical_uncertainty(self, phase: str, has_code: bool) -> float:
        """Calculate technical uncertainty"""
        base_uncertainty = {
            'ideation': 0.9,
            'design': 0.7,
            'mvp': 0.5,
            'implementation': 0.4,
            'testing': 0.2
        }.get(phase, 0.5)

        # Reduce if we have code
        if has_code:
            base_uncertainty *= 0.7

        return base_uncertainty

    def _calc_market_uncertainty(self, phase: str, validation: float) -> float:
        """Calculate market uncertainty"""
        if phase == 'ideation':
            return 0.9 * (1 - validation)
        elif phase == 'design':
            return 0.7 * (1 - validation)
        else:
            return 0.5 * (1 - validation)

    def _calc_resource_uncertainty(self, team_size: int, timeline_weeks: int) -> float:
        """Calculate resource uncertainty"""
        # Small team = higher uncertainty
        team_factor = 1.0 / math.sqrt(team_size)

        # Short timeline = higher uncertainty
        timeline_factor = 12.0 / timeline_weeks if timeline_weeks > 0 else 1.0

        return min(1.0, team_factor * timeline_factor * 0.7)

    def _calc_timeline_uncertainty(self, phase: str, timeline_weeks: int) -> float:
        """Calculate timeline uncertainty"""
        phase_factors = {
            'ideation': 0.3,
            'design': 0.5,
            'mvp': 0.7,
            'implementation': 0.8,
            'testing': 0.4
        }

        base = phase_factors.get(phase, 0.5)

        # Urgent timelines increase uncertainty
        if timeline_weeks < 4:
            return min(1.0, base * 2)
        elif timeline_weeks < 8:
            return min(1.0, base * 1.5)
        else:
            return base

    def _calc_quality_uncertainty(self, phase: str, has_code: bool) -> float:
        """Calculate quality uncertainty"""
        if not has_code and phase in ['implementation', 'testing']:
            return 0.9  # High uncertainty without code

        return {
            'ideation': 0.4,
            'design': 0.5,
            'mvp': 0.7,
            'implementation': 0.6,
            'testing': 0.3
        }.get(phase, 0.5)

    def add_observation(self, phase: str, vector: UncertaintyVector, outcome: bool):
        """
        Add observation for ML learning

        Args:
            phase: Current development phase
            vector: Uncertainty vector
            outcome: Success (True) or failure (False)
        """
        # Store observation for future training
        observation = {
            'timestamp': datetime.now().isoformat(),
            'phase': phase,
            'vector': asdict(vector),
            'outcome': outcome
        }

        # Add to patterns for learning
        phase_key = f'observations_{phase}'
        if phase_key not in self.patterns:
            self.patterns[phase_key] = []
        self.patterns[phase_key].append(observation)

        logger.debug("Added observation for phase %s: outcome=%s", phase, outcome)

    def classify_state(self, magnitude: float) -> UncertaintyState:
        """
        Classify uncertainty state based on magnitude

        Args:
            magnitude: Uncertainty magnitude (0-1)

        Returns:
            UncertaintyState enum value
        """
        if magnitude < 0.1:
            return UncertaintyState.DETERMINISTIC
        elif magnitude < 0.3:
            return UncertaintyState.PROBABILISTIC
        elif magnitude < 0.6:
            return UncertaintyState.QUANTUM
        elif magnitude < 0.8:
            return UncertaintyState.CHAOTIC
        else:
            return UncertaintyState.VOID

    def predict_evolution(self, vector: UncertaintyVector, phase: Optional[str] = None, hours: int = 24) -> PredictiveModel:
        """
        Predict how uncertainty will evolve

        Args:
            vector: Current uncertainty vector
            phase: Development phase (optional, for context)
            hours: Hours ahead to predict (default: 24)
        """
        # Analyze historical patterns
        pattern = self._match_pattern(vector)

        ml_prediction = self._predict_with_ml(vector, hours)
        if ml_prediction:
            trend, velocity, predicted_resolution, predicted_level = ml_prediction
        else:
            trend, velocity, predicted_resolution, predicted_level = self._predict_with_rules(
                vector,
                pattern,
                hours
            )

        lower_bound = max(0, min(vector.magnitude(), predicted_level) - 0.2)
        upper_bound = min(1, max(vector.magnitude(), predicted_level) + 0.2)

        # Create predictive model
        model = PredictiveModel(
            trend=trend,
            velocity=velocity,
            acceleration=-0.0001,  # Tend toward stability
            inflection_points=[],
            confidence_interval=(lower_bound, upper_bound),
            predicted_resolution=predicted_resolution
        )

        return model

    def _match_pattern(self, vector: UncertaintyVector) -> Optional[Dict]:
        """Match current vector to known patterns"""
        best_match = None
        best_distance = float('inf')

        for name, pattern in self.known_patterns.items():
            typical = pattern['typical_vector']

            # Calculate Euclidean distance
            distance = math.sqrt(
                (vector.technical - typical.technical)**2 +
                (vector.market - typical.market)**2 +
                (vector.resource - typical.resource)**2 +
                (vector.timeline - typical.timeline)**2 +
                (vector.quality - typical.quality)**2
            )

            if distance < best_distance:
                best_distance = distance
                best_match = pattern

        # Threshold for match
        if best_distance < 0.3:
            return best_match
        return None

    def generate_mitigations(self, vector: UncertaintyVector, state: UncertaintyState) -> List[MitigationStrategy]:
        """
        Auto-generate mitigation strategies
        """
        strategies = []
        uid = hashlib.md5(str(vector).encode()).hexdigest()[:8]

        # Get dominant dimension
        dominant = vector.dominant_dimension()

        # Generate strategies based on dominant uncertainty
        if dominant == 'technical':
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_1",
                uncertainty_id=uid,
                action="Conduct technical proof of concept",
                priority=1,
                estimated_impact=0.4,
                estimated_cost=16,  # hours
                prerequisites=[],
                success_probability=0.8,
                fallback_strategy="Consult external expert"
            ))
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_2",
                uncertainty_id=uid,
                action="Research similar implementations",
                priority=2,
                estimated_impact=0.3,
                estimated_cost=8,
                prerequisites=[],
                success_probability=0.9,
                fallback_strategy=None
            ))

        elif dominant == 'market':
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_3",
                uncertainty_id=uid,
                action="Conduct user interviews (10+)",
                priority=1,
                estimated_impact=0.5,
                estimated_cost=20,
                prerequisites=["Identify target users"],
                success_probability=0.7,
                fallback_strategy="Run online survey"
            ))
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_4",
                uncertainty_id=uid,
                action="Build MVP for validation",
                priority=2,
                estimated_impact=0.6,
                estimated_cost=40,
                prerequisites=["Core features defined"],
                success_probability=0.6,
                fallback_strategy="Create landing page"
            ))

        elif dominant == 'resource':
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_5",
                uncertainty_id=uid,
                action="Hire contractor/freelancer",
                priority=1,
                estimated_impact=0.5,
                estimated_cost=8,  # Time to find
                prerequisites=["Budget approval"],
                success_probability=0.7,
                fallback_strategy="Reduce scope"
            ))

        elif dominant == 'timeline':
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_6",
                uncertainty_id=uid,
                action="Implement parallel development",
                priority=1,
                estimated_impact=0.4,
                estimated_cost=4,
                prerequisites=["Clear task separation"],
                success_probability=0.8,
                fallback_strategy="Cut non-essential features"
            ))

        elif dominant == 'quality':
            strategies.append(MitigationStrategy(
                id=f"mit_{uid}_7",
                uncertainty_id=uid,
                action="Set up automated testing",
                priority=1,
                estimated_impact=0.6,
                estimated_cost=12,
                prerequisites=["Test framework chosen"],
                success_probability=0.9,
                fallback_strategy="Manual testing checklist"
            ))

        # Sort by ROI
        strategies.sort(key=lambda s: s.roi(), reverse=True)

        return strategies[:3]  # Return top 3 strategies

    def visualize_map(self, vector: UncertaintyVector, state: UncertaintyState) -> str:
        """
        Create ASCII visualization of uncertainty map
        """
        mag = vector.magnitude()

        def render_bar(value: float, length: int = 10) -> str:
            filled = max(0, min(length, int(round(value * length))))
            empty = length - filled
            return f"[{'=' * filled}{'.' * empty}]"

        bars = {
            'Technical': render_bar(vector.technical),
            'Market': render_bar(vector.market),
            'Resource': render_bar(vector.resource),
            'Timeline': render_bar(vector.timeline),
            'Quality': render_bar(vector.quality)
        }

        state_icons = {
            UncertaintyState.DETERMINISTIC: "OK",
            UncertaintyState.PROBABILISTIC: "VAR",
            UncertaintyState.QUANTUM: "QNT",
            UncertaintyState.CHAOTIC: "RISK",
            UncertaintyState.VOID: "UNK"
        }

        border = "+" + "-" * 52 + "+"

        def pad(content: str) -> str:
            return f"| {content.ljust(50)} |"

        lines = [
            border,
            pad("UNCERTAINTY MAP v3.0"),
            border,
            pad(f"State: {state_icons[state]} {state.value}"),
            pad(f"Magnitude: {mag:.1%} {render_bar(mag, 20)}"),
            border,
            pad("Dimensions"),
            pad(f"Technical : {bars['Technical']} ({vector.technical:.0%})"),
            pad(f"Market    : {bars['Market']} ({vector.market:.0%})"),
            pad(f"Resource  : {bars['Resource']} ({vector.resource:.0%})"),
            pad(f"Timeline  : {bars['Timeline']} ({vector.timeline:.0%})"),
            pad(f"Quality   : {bars['Quality']} ({vector.quality:.0%})"),
            border
        ]

        return "\n".join(lines)

    def save_state(self, filepath: Optional[Path] = None):
        """Save current state to file"""
        state = {
            'project': self.project_name,
            'timestamp': datetime.now().isoformat(),
            'uncertainties': {k: asdict(v) for k, v in self.uncertainties.items()},
            'patterns': self.patterns
        }

        target = Path(filepath).expanduser() if filepath else self.history_file
        target.parent.mkdir(parents=True, exist_ok=True)
        lock = FileLock(str(target) + '.lock')
        with lock:
            with open(target, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)

    def load_history(self):
        """Load historical data"""
        if self.history_file.exists():
            lock = FileLock(str(self.history_file) + '.lock')
            with lock:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.patterns = data.get('patterns', {})


def demo():
    """Demo the Uncertainty Map v3"""
    logging.basicConfig(level=logging.INFO)
    logger.info("%s", "=" * 60)
    logger.info("Uncertainty Map v3.0 Demo")
    logger.info("%s", "=" * 60)

    # Create map
    umap = UncertaintyMapV3("2025-Revenue-App")

    # Test different contexts
    contexts = [
        {
            'name': 'Ideation Phase',
            'phase': 'ideation',
            'files': [],
            'team_size': 5,
            'timeline_weeks': 12,
            'market_validation': 0.2
        },
        {
            'name': 'Implementation Phase',
            'phase': 'implementation',
            'files': ['app.py', 'models.py', 'views.py'],
            'team_size': 5,
            'timeline_weeks': 8,
            'market_validation': 0.6
        }
    ]

    for ctx in contexts:
        logger.info("Context: %s", ctx['name'])
        logger.info("%s", "-" * 50)

        # Analyze
        vector, state = umap.analyze_context(ctx)

        # Visualize
        logger.info("\n%s", umap.visualize_map(vector, state))

        # Predict
        prediction = umap.predict_evolution(vector)
        logger.info("Prediction:")
        logger.info("Trend: %s", prediction.trend)
        logger.info("24h forecast: %.1f%%", prediction.predict_future(24) * 100)
        logger.info("Resolution: %s", prediction.predicted_resolution)

        # Mitigations
        mitigations = umap.generate_mitigations(vector, state)
        logger.info("Top mitigation strategies:")
        for i, mit in enumerate(mitigations, 1):
            logger.info("%d. %s", i, mit.action)
            logger.info(
                "Impact: %.1f%%, Cost: %sh, ROI: %.1f",
                mit.estimated_impact * 100,
                mit.estimated_cost,
                mit.roi()
            )



class Uncertainty:
    """
    Facade for UncertaintyMapV3 to match simple test API.
    Used for MVP testing and backward compatibility.
    """
    def __init__(self):
        self.engine = UncertaintyMapV3("default_project")
        # Initialize with a default context to have a baseline vector
        self.default_vector = UncertaintyVector(0.5, 0.5, 0.5, 0.5, 0.5)

    def predict(self, hours_ahead: int) -> Dict[str, Any]:
        """
        Simple prediction API for testing.
        Returns dictionary of {area: Prediction(level=...)}
        """
        if hours_ahead < 0:
            raise ValueError("hours_ahead must be non-negative")

        # Create a dummy result structure
        # In a real scenario, we would predict for each area separately if tracked separately.
        # Here we simulate it based on dimensions.

        # Use V3 evolution logic
        model = self.engine.predict_evolution(self.default_vector, hours=hours_ahead)
        future_level = model.predict_future(hours_ahead)

        # Structure matching test expectations (dict of objects with .level)
        @dataclass
        class SimplePrediction:
            level: float
            trend: str

        result = {
            "global": SimplePrediction(level=future_level, trend=model.trend),
            "technical": SimplePrediction(level=self.default_vector.technical, trend="stable"),  # Simplified
            "market": SimplePrediction(level=self.default_vector.market, trend="stable")
        }

        # Week 0 Day 3: Log predictions for ground truth validation
        self._log_prediction_for_validation(hours_ahead, result)

        return result

    def _log_prediction_for_validation(self, hours_ahead: int, result: Dict[str, Any]) -> None:
        """Log prediction for future ground truth validation (Week 0 Day 3)"""
        try:
            prediction_timestamp = datetime.now()
            validation_timestamp = prediction_timestamp + timedelta(hours=hours_ahead)

            # Helper to determine state from level
            def level_to_state(level: float) -> str:
                if level < 0.10:
                    return "DETERMINISTIC"
                elif level < 0.30:
                    return "PROBABILISTIC"
                elif level < 0.60:
                    return "QUANTUM"
                elif level < 0.90:
                    return "CHAOTIC"
                else:
                    return "VOID"

            log_entry = {
                "prediction_timestamp": prediction_timestamp.isoformat(),
                "validation_timestamp": validation_timestamp.isoformat(),
                "hours_ahead": hours_ahead,
                "predicted_global_level": result["global"].level,
                "predicted_global_trend": result["global"].trend,
                "predicted_global_state": level_to_state(result["global"].level),
                "previous_level": self.default_vector.magnitude()  # For trend calculation
            }

            # Write to predictions log
            log_file = DEFAULT_STORAGE_DIR / "predictions_log.jsonl"

            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry) + "\n")

            logger.debug(f"Logged prediction: {hours_ahead}h ahead, level={result['global'].level:.2%}")

        except Exception as e:
            # Don't fail prediction if logging fails
            logger.warning(f"Failed to log prediction for validation: {e}")

if __name__ == "__main__":
    demo()
