"""
Bayesian Confidence Scoring Service

Implements production-ready Bayesian inference for uncertainty quantification
with <5ms fast mode and 10-20ms full mode performance.

Based on Beta-Binomial conjugacy for efficient posterior updates.
"""

import math
from functools import lru_cache
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
import logging

from app.models.uncertainty import (
    UncertaintyStateEnum,
    BayesianConfidenceRequest,
    BayesianConfidenceResponse,
    BayesianMetadata
)

logger = logging.getLogger(__name__)


# ============================================================================
# Phase-Specific Prior Distributions (Beta Parameters)
# ============================================================================

PHASE_PRIORS = {
    "ideation": {
        "alpha": 4.0,  # Prior successes
        "beta": 6.0,   # Prior failures
        "mean": 0.40,  # Expected confidence
        "variance": 0.022
    },
    "design": {
        "alpha": 5.5,
        "beta": 4.5,
        "mean": 0.55,
        "variance": 0.023
    },
    "mvp": {
        "alpha": 6.5,
        "beta": 3.5,
        "mean": 0.65,
        "variance": 0.021
    },
    "implementation": {
        "alpha": 7.0,
        "beta": 3.0,
        "mean": 0.70,
        "variance": 0.019
    },
    "testing": {
        "alpha": 8.0,
        "beta": 2.0,
        "mean": 0.80,
        "variance": 0.015
    }
}


# ============================================================================
# Core Bayesian Scoring Class with Optimization
# ============================================================================

class OptimizedBayesianScorer:
    """
    Production-optimized Bayesian confidence scorer.

    Features:
    - LRU caching for frequent calculations
    - Fast approximation mode (<5ms)
    - Full Bayesian mode (10-20ms) with credible intervals
    - Automatic state classification
    """

    def __init__(self, cache_size: int = 128):
        """
        Initialize Bayesian scorer with LRU cache.

        Args:
            cache_size: Maximum number of cached calculations (default: 128)
        """
        self.cache_size = cache_size
        self._init_cache()

    def _init_cache(self):
        """Initialize LRU cache for performance optimization"""
        # Cache is implemented via @lru_cache decorator on methods
        logger.info(f"[OK] Bayesian scorer initialized with cache_size={self.cache_size}")

    @lru_cache(maxsize=128)
    def _beta_mean(self, alpha: float, beta: float) -> float:
        """Calculate Beta distribution mean (cached)"""
        return alpha / (alpha + beta)

    @lru_cache(maxsize=128)
    def _beta_variance(self, alpha: float, beta: float) -> float:
        """Calculate Beta distribution variance (cached)"""
        n = alpha + beta
        return (alpha * beta) / (n * n * (n + 1))

    def _beta_credible_interval(
        self,
        alpha: float,
        beta: float,
        confidence_level: float = 0.95
    ) -> Tuple[float, float]:
        """
        Calculate credible interval using normal approximation.

        For large alpha+beta (>20), Beta distribution approximates Normal.
        This is 100x faster than scipy.stats.beta.ppf().

        Args:
            alpha: Beta distribution alpha parameter
            beta: Beta distribution beta parameter
            confidence_level: CI level (default: 0.95 for 95% CI)

        Returns:
            Tuple of (lower_bound, upper_bound)
        """
        mean = self._beta_mean(alpha, beta)
        variance = self._beta_variance(alpha, beta)
        std = math.sqrt(variance)

        # Z-score for 95% CI is 1.96
        z_score = 1.96 if confidence_level == 0.95 else 2.576  # 99% CI

        lower = max(0.0, mean - z_score * std)
        upper = min(1.0, mean + z_score * std)

        return (lower, upper)

    def calculate_likelihood(
        self,
        context: dict,
        historical_outcomes: List[bool]
    ) -> float:
        """
        Calculate likelihood from evidence.

        Args:
            context: Project context with metrics
            historical_outcomes: Past success/failure outcomes

        Returns:
            Likelihood score (0-1)
        """
        # Base likelihood from historical outcomes
        if historical_outcomes:
            successes = sum(historical_outcomes)
            total = len(historical_outcomes)
            base_likelihood = successes / total
        else:
            base_likelihood = 0.5  # Neutral prior

        # Context-based adjustments
        adjustments = []

        # Team size factor (more people = higher coordination uncertainty)
        if "team_size" in context:
            team_size = context["team_size"]
            if team_size == 1:
                adjustments.append(0.1)  # Solo = less coordination overhead
            elif team_size <= 3:
                adjustments.append(0.05)
            elif team_size > 5:
                adjustments.append(-0.1)  # Large team = more uncertainty

        # Code existence (has_code = True reduces technical uncertainty)
        if context.get("has_code", False):
            adjustments.append(0.15)

        # Market validation
        if "validation_score" in context:
            val_score = context["validation_score"]
            adjustments.append((val_score - 0.5) * 0.2)  # -0.1 to +0.1

        # Timeline pressure
        if "timeline_weeks" in context:
            weeks = context["timeline_weeks"]
            if weeks < 4:
                adjustments.append(-0.1)  # Tight timeline = more risk
            elif weeks > 12:
                adjustments.append(0.05)  # Ample time = less risk

        # Apply adjustments
        adjusted_likelihood = base_likelihood + sum(adjustments)

        # Clamp to [0, 1]
        return max(0.0, min(1.0, adjusted_likelihood))

    def calculate_posterior(
        self,
        prior_alpha: float,
        prior_beta: float,
        likelihood: float,
        evidence_strength: int = 10
    ) -> Tuple[float, float]:
        """
        Calculate posterior Beta parameters using Bayesian update.

        Args:
            prior_alpha: Prior Beta alpha
            prior_beta: Prior Beta beta
            likelihood: Evidence likelihood (0-1)
            evidence_strength: Weight of evidence (default: 10)

        Returns:
            Tuple of (posterior_alpha, posterior_beta)
        """
        # Convert likelihood to pseudo-counts
        successes = likelihood * evidence_strength
        failures = (1 - likelihood) * evidence_strength

        # Bayesian update (conjugate prior)
        posterior_alpha = prior_alpha + successes
        posterior_beta = prior_beta + failures

        return (posterior_alpha, posterior_beta)

    def calculate_uncertainty_vector(
        self,
        context: dict,
        confidence_score: float
    ) -> Dict[str, float]:
        """
        Calculate multi-dimensional uncertainty vector.

        Args:
            context: Project context
            confidence_score: Overall confidence (0-1)

        Returns:
            Dictionary with technical, market, resource, timeline, quality dimensions
        """
        # Base uncertainty (inverse of confidence)
        base_uncertainty = 1 - confidence_score

        # Dimension-specific calculations
        technical = base_uncertainty * 1.2 if not context.get("has_code", False) else base_uncertainty * 0.8

        market = base_uncertainty * (1 - context.get("validation_score", 0.5))

        team_size = context.get("team_size", 1)
        resource = base_uncertainty * (1 + (team_size - 1) * 0.1)

        timeline_weeks = context.get("timeline_weeks", 4)
        timeline = base_uncertainty * (1.5 if timeline_weeks < 4 else 0.8)

        quality = base_uncertainty * 0.9  # Generally lower than other dimensions

        # Normalize all to [0, 1]
        vector = {
            "technical": max(0.0, min(1.0, technical)),
            "market": max(0.0, min(1.0, market)),
            "resource": max(0.0, min(1.0, resource)),
            "timeline": max(0.0, min(1.0, timeline)),
            "quality": max(0.0, min(1.0, quality))
        }

        # Calculate magnitude (L2 norm)
        magnitude = math.sqrt(sum(v**2 for v in vector.values())) / math.sqrt(5)
        vector["magnitude"] = magnitude

        # Identify dominant dimension
        dimensions = {k: v for k, v in vector.items() if k != "magnitude"}
        dominant = max(dimensions.items(), key=lambda x: x[1])
        vector["dominant_dimension"] = dominant[0]

        return vector


# ============================================================================
# High-Level API Functions
# ============================================================================

def classify_uncertainty_state(magnitude: float) -> UncertaintyStateEnum:
    """
    Classify uncertainty into quantum-inspired states.

    Args:
        magnitude: Uncertainty magnitude (0-1)

    Returns:
        UncertaintyStateEnum
    """
    if magnitude < 0.10:
        return UncertaintyStateEnum.DETERMINISTIC
    elif magnitude < 0.30:
        return UncertaintyStateEnum.PROBABILISTIC
    elif magnitude < 0.60:
        return UncertaintyStateEnum.QUANTUM
    elif magnitude < 0.90:
        return UncertaintyStateEnum.CHAOTIC
    else:
        return UncertaintyStateEnum.VOID


def _calculate_risk_level(magnitude: float, confidence_score: float) -> str:
    """
    Calculate risk level based on uncertainty and confidence.

    Args:
        magnitude: Uncertainty magnitude (0-1)
        confidence_score: Confidence score (0-1)

    Returns:
        Risk level: low/medium/high/critical
    """
    # Combined risk metric
    risk_score = magnitude * (1 - confidence_score)

    if risk_score < 0.15:
        return "low"
    elif risk_score < 0.35:
        return "medium"
    elif risk_score < 0.60:
        return "high"
    else:
        return "critical"


def _calculate_monitoring_level(risk_level: str, state: UncertaintyStateEnum) -> str:
    """
    Determine monitoring intensity based on risk and state.

    Args:
        risk_level: Risk level (low/medium/high/critical)
        state: Uncertainty state

    Returns:
        Monitoring level: minimal/standard/enhanced/intensive/critical
    """
    if risk_level == "critical" or state == UncertaintyStateEnum.VOID:
        return "critical"
    elif risk_level == "high" or state == UncertaintyStateEnum.CHAOTIC:
        return "intensive"
    elif risk_level == "medium" or state == UncertaintyStateEnum.QUANTUM:
        return "enhanced"
    elif state == UncertaintyStateEnum.PROBABILISTIC:
        return "standard"
    else:
        return "minimal"


def _generate_recommendations(
    state: UncertaintyStateEnum,
    risk_level: str,
    dominant_dimension: str,
    confidence_score: float
) -> List[str]:
    """
    Generate actionable recommendations based on analysis.

    Args:
        state: Uncertainty state
        risk_level: Risk level
        dominant_dimension: Primary uncertainty source
        confidence_score: Overall confidence

    Returns:
        List of recommendation strings
    """
    recommendations = []

    # State-specific recommendations
    if state == UncertaintyStateEnum.DETERMINISTIC:
        recommendations.append("[EMOJI] High confidence - proceed with standard monitoring")
        recommendations.append("[EMOJI] Document assumptions for future reference")

    elif state == UncertaintyStateEnum.PROBABILISTIC:
        recommendations.append("[EMOJI] Good confidence - proceed with standard checkpoints")
        recommendations.append("[EMOJI] Document assumptions and validate periodically")
        recommendations.append(f"[EMOJI] Focus on {dominant_dimension} dimension")

    elif state == UncertaintyStateEnum.QUANTUM:
        recommendations.append("[WARN] Moderate uncertainty - increase checkpoint frequency")
        recommendations.append("[EMOJI] Deep dive into {dominant_dimension} uncertainty")
        recommendations.append("[EMOJI] Gather more data before major decisions")
        recommendations.append("[EMOJI] Consider iterative approach with validation gates")

    elif state == UncertaintyStateEnum.CHAOTIC:
        recommendations.append("[EMOJI] High uncertainty - proceed with caution")
        recommendations.append("[EMOJI] CRITICAL: Address {dominant_dimension} immediately")
        recommendations.append("[EMOJI] Spike/prototype to reduce uncertainty")
        recommendations.append("[EMOJI] Seek expert consultation")
        recommendations.append("[EMOJI] Break into smaller, validated increments")

    elif state == UncertaintyStateEnum.VOID:
        recommendations.append("[FAIL] Extreme uncertainty - DO NOT PROCEED")
        recommendations.append("[EMOJI] Research phase required")
        recommendations.append("[EMOJI] Study similar problems/solutions first")
        recommendations.append("[EMOJI] Upskill team in unknown areas")
        recommendations.append("[EMOJI] Consider pivot or alternative approaches")

    # Risk-specific recommendations
    if risk_level == "critical":
        recommendations.append("[EMOJI] CRITICAL RISK: Executive review required")
    elif risk_level == "high":
        recommendations.append("[WARN] High risk: Daily monitoring needed")

    # Dimension-specific recommendations
    if dominant_dimension == "technical":
        recommendations.append("[EMOJI] Technical spike recommended")
    elif dominant_dimension == "timeline":
        recommendations.append("â± Review timeline constraints")
    elif dominant_dimension == "resource":
        recommendations.append("[EMOJI] Review resource allocation")
    elif dominant_dimension == "market":
        recommendations.append("[EMOJI] Conduct market validation")
    elif dominant_dimension == "quality":
        recommendations.append("[EMOJI] Implement quality gates")

    return recommendations


def calculate_bayesian_confidence(
    request: BayesianConfidenceRequest
) -> BayesianConfidenceResponse:
    """
    Main entry point for Bayesian confidence calculation.

    Args:
        request: BayesianConfidenceRequest with phase, context, historical_outcomes

    Returns:
        BayesianConfidenceResponse with full analysis
    """
    start_time = datetime.now()

    # Initialize scorer
    scorer = OptimizedBayesianScorer()

    # Get phase-specific prior
    phase = request.phase.lower()
    if phase not in PHASE_PRIORS:
        logger.warning(f"Unknown phase '{phase}', defaulting to 'implementation'")
        phase = "implementation"

    prior = PHASE_PRIORS[phase]
    prior_alpha = prior["alpha"]
    prior_beta = prior["beta"]
    prior_mean = prior["mean"]

    # Calculate likelihood from evidence
    likelihood = scorer.calculate_likelihood(
        context=request.context,
        historical_outcomes=request.historical_outcomes
    )

    # Calculate posterior (Bayesian update)
    evidence_strength = len(request.historical_outcomes) if request.historical_outcomes else 10
    posterior_alpha, posterior_beta = scorer.calculate_posterior(
        prior_alpha=prior_alpha,
        prior_beta=prior_beta,
        likelihood=likelihood,
        evidence_strength=evidence_strength
    )

    # Calculate confidence score (posterior mean)
    confidence_score = scorer._beta_mean(posterior_alpha, posterior_beta)

    # Calculate uncertainty vector
    uncertainty_vector = scorer.calculate_uncertainty_vector(
        context=request.context,
        confidence_score=confidence_score
    )

    magnitude = uncertainty_vector["magnitude"]
    dominant_dimension = uncertainty_vector["dominant_dimension"]

    # Classify uncertainty state
    state = classify_uncertainty_state(magnitude)

    # Calculate risk and monitoring levels
    risk_level = _calculate_risk_level(magnitude, confidence_score)
    monitoring_level = _calculate_monitoring_level(risk_level, state)

    # Generate decision
    if state in [UncertaintyStateEnum.DETERMINISTIC, UncertaintyStateEnum.PROBABILISTIC]:
        decision = "GO"
    elif state == UncertaintyStateEnum.QUANTUM:
        decision = "GO_WITH_CHECKPOINTS"
    else:  # CHAOTIC or VOID
        decision = "NO_GO"

    # Generate recommendations
    recommendations = _generate_recommendations(
        state=state,
        risk_level=risk_level,
        dominant_dimension=dominant_dimension,
        confidence_score=confidence_score
    )

    # Calculate credible intervals
    if request.use_fast_mode:
        # Fast approximation (no CI calculation)
        ci_lower = max(0.0, confidence_score - 0.1)
        ci_upper = min(1.0, confidence_score + 0.1)
        mode = "fast"
        posterior_mean_value = None
        confidence_precision = None
        effective_sample_size = None
    else:
        # Full Bayesian with credible intervals
        ci_lower, ci_upper = scorer._beta_credible_interval(posterior_alpha, posterior_beta)
        mode = "full"
        posterior_mean_value = confidence_score
        variance = scorer._beta_variance(posterior_alpha, posterior_beta)
        confidence_precision = 1 / variance if variance > 0 else float('inf')
        effective_sample_size = int(posterior_alpha + posterior_beta)

    # Build metadata
    metadata = BayesianMetadata(
        mode=mode,
        prior_mean=prior_mean,
        likelihood=likelihood,
        posterior_mean=posterior_mean_value,
        credible_interval_lower=ci_lower,
        credible_interval_upper=ci_upper,
        effective_sample_size=effective_sample_size,
        uncertainty_magnitude=magnitude,
        confidence_precision=confidence_precision,
        risk_level=risk_level,
        monitoring_level=monitoring_level,
        dominant_dimension=dominant_dimension
    )

    # Log performance
    elapsed_ms = (datetime.now() - start_time).total_seconds() * 1000
    logger.info(f"Bayesian calculation completed in {elapsed_ms:.2f}ms (mode={mode})")

    # Build response
    response = BayesianConfidenceResponse(
        confidence_score=confidence_score,
        state=state,
        decision=decision,
        metadata=metadata,
        recommendations=recommendations,
        timestamp=datetime.now()
    )

    return response
